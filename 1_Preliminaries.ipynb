{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to load and pre-process data from the [COCO dataset](http://cocodataset.org/#home). You will also explore how to design a CNN-RNN model for automatically generating image captions.\n",
    "\n",
    "This notebook is designed to introduce you to the helper files that you will use to complete your project. \n",
    "\n",
    "Note that **any amendments that you make to this notebook will NOT be graded**.\n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Instantiate the Data Loader\n",
    "- [Step 2](#step2): Use the Data Loader to Obtain Batches\n",
    "- [Step 3](#step3): Tinker with the CNN Encoder\n",
    "- [Step 4](#step4): Tinker with the RNN Decoder\n",
    "- [Step 5](#step5): Calculate the Loss\n",
    "- [Step 6](#step6): Celebrate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Explore the Data Loader\n",
    "\n",
    "We have already written a [data loader](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) that you can use to load the COCO dataset in batches. \n",
    "\n",
    "In the code cell below, you will initialize the data loader by using the `get_loader` function in **data_loader.py**.  \n",
    "\n",
    "> For this project, you are not permitted to change the **data_loader.py** file, which must be used as-is.\n",
    "\n",
    "The `get_loader` function takes as input a number of arguments that can be explored in **data_loader.py**.  Take the time to explore these arguments now by opening **data_loader.py** in a new window.  For now, we need only focus on four arguments:\n",
    "1. **`transform`** - an [image transform](http://pytorch.org/docs/master/torchvision/transforms.html) specifying how to pre-process the images and convert them to PyTorch tensors before using them as input to the CNN encoder.  For now, you are encouraged to keep the transform as provided in `transform_train`.  You will have the opportunity later to choose your own image transform to pre-process the COCO images.\n",
    "2. **`mode`** - one of `'train'` (loads the training data in batches), `'val'` (loads the validation data), or `'test'` (for the test data). We will say that the data loader is in training, validation, or test mode, respectively.\n",
    "3. **`batch_size`** - determines the batch size.  If training the model, this is total number of image-caption pairs used to amend the model weights in each training step.\n",
    "4. **`vocab_threshold`** - the total number of times that a word must appear in the in the training captions before it is used as part of the vocabulary.  Images that have fewer than `vocab_threshold` occurrences are considered unknown words. \n",
    "\n",
    "We will further detail the `vocab_threshold` and `vocab_from_file` arguments soon.  For now, run the code cell below.  Be patient - it may take a couple of minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data_loader import get_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a transform to pre-process the training images.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Set the minimum word count threshold.\n",
    "vocab_threshold = 5\n",
    "\n",
    "# Specify the batch size.\n",
    "batch_size = 10\n",
    "\n",
    "# Obtain the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you ran the code cell above, the data loader was stored in the variable `data_loader`.  \n",
    "\n",
    "You can access the corresponding dataset as `data_loader.dataset`.  This dataset is an instance of the `CoCoDataset` class in **data_loader.py**.  If you are unfamiliar with data loaders and datasets, you are encouraged to review [this PyTorch tutorial](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "### Exploring the `__getitem__` Method\n",
    "\n",
    "The `__getitem__` method in the `CoCoDataset` class determines how a data sample (or *image-caption pair*) is pre-processed before being incorporated into a batch.  When the data loader is in training mode, this method begins by first obtaining the image path (`path`) and caption (`caption`).\n",
    "\n",
    "#### Image Pre-Processing \n",
    "\n",
    "Image pre-processing is relatively straightforward (from the `__getitem__` method in the `CoCoDataset` class):\n",
    "```python\n",
    "# Convert image to tensor and pre-process using transform\n",
    "image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "image = self.transform(image)\n",
    "```\n",
    "After loading the image located at `path`, the image is pre-processed using the same transform that was supplied when instantiating the data loader. \n",
    "\n",
    "The pre-processing of the caption is relatively more involved and is covered in the following section.\n",
    "\n",
    "#### Caption Pre-Processing \n",
    "\n",
    "To understand how COCO captions are pre-processed, we'll first need to take a look at the `vocab` instance variable of the `CoCoDataset` class (from the `__init__` method of the `CoCoDataset` class):\n",
    "```python\n",
    "def __init__(self, transform, mode, batch_size, vocab_threshold, vocab_file, start_word, \n",
    "        end_word, unk_word, annotations_file, vocab_from_file, img_folder):\n",
    "        ...\n",
    "        self.vocab = Vocabulary(vocab_threshold, vocab_file, start_word,\n",
    "            end_word, unk_word, annotations_file, vocab_from_file)\n",
    "        ...\n",
    "```\n",
    "Take the time now to peruse the code to check that `data_loader.dataset.vocab` is an instance of the `Vocabulary` class in **vocabulary.py**.  \n",
    "\n",
    "We will use an instance of this class to pre-process the COCO captions by mapping tokenized captions to lists of integers (from the `__getitem__` method in the `CoCoDataset` class):\n",
    "\n",
    "```python\n",
    "# Convert caption to tensor of word ids.\n",
    "tokens = nltk.tokenize.word_tokenize(str(caption).lower())   # line 1\n",
    "caption = []                                                 # line 2\n",
    "caption.append(self.vocab(self.vocab.start_word))            # line 3\n",
    "caption.extend([self.vocab(token) for token in tokens])      # line 4\n",
    "caption.append(self.vocab(self.vocab.end_word))              # line 5\n",
    "caption = torch.Tensor(caption).long()                       # line 6\n",
    "```\n",
    "\n",
    "To see how this code works, we'll apply it to the sample caption in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_caption = 'The golden retriever is running fast.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **`line 1`** of the code snippet, every letter in the caption is converted to lowercase, and the [`nltk.tokenize.word_tokenize`](http://www.nltk.org/) function is used to obtain a list of string-valued tokens.  Run the next code cell to visualize the effect on `sample_caption`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "sample_tokens = nltk.tokenize.word_tokenize(str(sample_caption).lower())\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **`line 2`** and **`line 3`** we initialize an empty list and append an integer that marks the beginning of a caption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_caption = []\n",
    "sample_caption.append(data_loader.dataset.vocab('<start>'))\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the integer `0` is always used to mark the beginning of a caption.\n",
    "\n",
    "Then, in **`line 4`**, we continue the list by adding integers that correspond to each of the tokens in the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_caption.extend([data_loader.dataset.vocab(token) for token in sample_tokens])\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **`line 5`**, we append a final integer to mark the end of the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_caption.append(data_loader.dataset.vocab('<end>'))\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the integer `1` is always used to mark the end of a caption. \n",
    "\n",
    "Finally, in **`line 6`**, we convert the list of integers to a PyTorch tensor and cast it to [long type](http://pytorch.org/docs/master/tensors.html#torch.Tensor.long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sample_caption = torch.Tensor(sample_caption).long()\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add a tl;dr description of the vocabulary.\n",
    "\n",
    "The total number of tokens can be counted by using the command in the following code cell.  All tokens mapped to a non-negative integer less than the total number of tokens ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tokens (which we will also refer to as the \"size of the vocabulary\") is created by looping over the captions in the training dataset.  If a token appears no less than `vocab_threshold` times in the training set, then it is added to the vocabulary.\n",
    "\n",
    "You will have the option later to amend the `vocab_threshold` argument when instantiating your data loader.  Note that in general, **larger** values for `vocab_threshold` yield a **smaller** number of tokens in the vocabulary.  You are encouraged to check this for yourself now by amending the value of `vocab_threshold` before creating a new data loader.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify the minimum word count threshold.\n",
    "vocab_threshold = 4\n",
    "\n",
    "# Obtain the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next code cell to check that the total number of words in the vocabulary has increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how all tokens that are not in the vocabulary are mapped to the same integer, considered unknown words ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data_loader.dataset.vocab('jfkafejw'))\n",
    "print(data_loader.dataset.vocab('ieowoqjf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training your model, you are encouraged to peruse the `vocabulary.py` file in greater detail.  In the next section, we'll focus on figuring out how to use the data loader to obtain batches of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Use the Data Loader to Obtain Batches\n",
    "\n",
    "During training, to generate batches of image-caption pairs, we begin by first sampling a caption length.  Then, we retrieve a batch of size `batch_size` with that length.  This approach for assembling batches matches the procedure in [this paper](https://arxiv.org/pdf/1502.03044.pdf), and has been shown to be computationally efficient without degrading performance.\n",
    "\n",
    "Make clear that they will see this code again when train the model - code is provided for them.\n",
    "\n",
    "Run the code cell multiple times to see different batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Randomly sample a caption length, and sample indices with that length.\n",
    "indices = data_loader.dataset.get_train_indices()\n",
    "print('sampled indices:', indices)\n",
    "# Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "data_loader.batch_sampler.sampler = new_sampler\n",
    "    \n",
    "# Obtain the batch.\n",
    "for batch in data_loader:\n",
    "    images, captions = batch[0], batch[1]\n",
    "    break\n",
    "    \n",
    "print('images.shape:',images.shape)\n",
    "print('captions.shape:', captions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: Tinker with the CNN Encoder\n",
    "\n",
    "- make note of module autoreload capability\n",
    "- need to explain to them what `volatile` is for, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "% autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    \"\"\" converts a Pytorch Tensor to a variable and moves to GPU if CUDA is available \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- provide them a CNN encoder.\n",
    "- let them decide if they want to swap out the pre-trained architecture for another (make clear this is allowed). show how to do for vgg-16\n",
    "- consider adding batch normalization if you like ...\n",
    "- make any modifications to the EncoderCNN class you like. use this as a playground to make sure that your model is working well.\n",
    "- Your output should be a Torch Variable with shape `[batch_size, embed_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model import EncoderCNN\n",
    "\n",
    "# TODO: Specify the dimensionality of the image embedding.\n",
    "embed_size = 256\n",
    "\n",
    "# Initialize the encoder.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "# Move the encoder to GPU if CUDA is available.\n",
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()\n",
    "    \n",
    "# Convert last batch of images (from Step 1) to PyTorch Variable.   \n",
    "images_var = to_var(images, volatile=True)\n",
    "# Pass the images through the encoder.\n",
    "features = encoder(images_var)\n",
    "\n",
    "print('type(features):', type(features))\n",
    "print('features.shape:', features.shape)\n",
    "\n",
    "# Check that your encoder satisfies the requirements of the project.\n",
    "if (type(features)==torch.autograd.variable.Variable) & (features.shape[0]==batch_size) & (features.shape[1]==embed_size):\n",
    "    print('\\nyou may proceed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "## Step 4: Tinker with the RNN Decoder\n",
    "\n",
    "- they have to write the decoder here and I plan to provide a unit test where they can check if implementation is correct\n",
    "- they have to decide hidden size. note embed_size must be same as with encoder, and output must be same as vocab size.\n",
    "- if leave notebook and come back, re-run every cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model import DecoderRNN\n",
    "\n",
    "# TODO: Specify the number of features in the hidden state of the RNN decoder.\n",
    "hidden_size = 512\n",
    "\n",
    "# Store the size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the decoder.\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "# Move the decoder to GPU if CUDA is available.\n",
    "if torch.cuda.is_available():\n",
    "    decoder = decoder.cuda()\n",
    "    \n",
    "# Convert last batch of captions (from Step 1) to a PyTorch Variable. \n",
    "captions_var = to_var(captions)\n",
    "# Pass the encoder output and captions through the decoder.\n",
    "outputs = decoder(features, captions_var)\n",
    "\n",
    "print('type(outputs):', type(outputs))\n",
    "print('outputs.shape:', outputs.shape)\n",
    "\n",
    "# Check that your decoder satisfies the requirements of the project.\n",
    "if (type(outputs)==torch.autograd.variable.Variable) & (outputs.shape[0]==batch_size) & (outputs.shape[1]==captions.shape[1]) & (outputs.shape[2]==vocab_size):\n",
    "    print('\\nyou may proceed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "## Step 5: Calculate the Loss\n",
    "\n",
    "- i give the criterion. students have to calculate the loss\n",
    "- still trying to figure out how to best assess this. maybe put in py file that's not hidden? tell students not to look unless they really need to? will appear in the next notebook anyway. may as well put it in a file that's not hidden ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: Calculate the batch loss.\n",
    "loss = criterion(outputs.view(-1, vocab_size), captions_var.view(-1))\n",
    "\n",
    "print('loss:', loss.data[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
