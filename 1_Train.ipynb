{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "In this notebook, some template code has already been provided for you ...\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Training Setup\n",
    "\n",
    "do NOT change the lines of code that are not preceded with a TODO statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.61s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 765/414113 [00:00<00:54, 7648.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:41<00:00, 10091.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    \"\"\" converts a Pytorch Tensor to a variable and moves to GPU if CUDA is available \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "## TODO: Select appropriate values for the Python variables below.\n",
    "batch_size = 128\n",
    "vocab_threshold = 4\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "save_every = 1\n",
    "\n",
    "# TODO: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) \n",
    "\n",
    "# TODO: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params=params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train your Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [0/3236], Loss: 4.9002, Perplexity: 134.3109\n",
      "Epoch [1/5], Step [1/3236], Loss: 4.8436, Perplexity: 126.9221\n",
      "Epoch [1/5], Step [2/3236], Loss: 4.6813, Perplexity: 107.9077\n",
      "Epoch [1/5], Step [3/3236], Loss: 4.8167, Perplexity: 123.5618\n",
      "Epoch [1/5], Step [4/3236], Loss: 4.8428, Perplexity: 126.8201\n",
      "Epoch [1/5], Step [5/3236], Loss: 4.8764, Perplexity: 131.1607\n",
      "Epoch [1/5], Step [6/3236], Loss: 4.7246, Perplexity: 112.6883\n",
      "Epoch [1/5], Step [7/3236], Loss: 4.6746, Perplexity: 107.1911\n",
      "Epoch [1/5], Step [8/3236], Loss: 4.5919, Perplexity: 98.6832\n",
      "Epoch [1/5], Step [9/3236], Loss: 4.6595, Perplexity: 105.5839\n",
      "Epoch [1/5], Step [10/3236], Loss: 4.7146, Perplexity: 111.5665\n",
      "Epoch [1/5], Step [11/3236], Loss: 5.6239, Perplexity: 276.9793\n",
      "Epoch [1/5], Step [12/3236], Loss: 4.6107, Perplexity: 100.5535\n",
      "Epoch [1/5], Step [13/3236], Loss: 4.5215, Perplexity: 91.9734\n",
      "Epoch [1/5], Step [14/3236], Loss: 4.5209, Perplexity: 91.9163\n",
      "Epoch [1/5], Step [15/3236], Loss: 4.5866, Perplexity: 98.1590\n",
      "Epoch [1/5], Step [16/3236], Loss: 4.5576, Perplexity: 95.3565\n",
      "Epoch [1/5], Step [17/3236], Loss: 4.5945, Perplexity: 98.9359\n",
      "Epoch [1/5], Step [18/3236], Loss: 4.7343, Perplexity: 113.7894\n",
      "Epoch [1/5], Step [19/3236], Loss: 4.4069, Perplexity: 82.0114\n",
      "Epoch [1/5], Step [20/3236], Loss: 4.6674, Perplexity: 106.4239\n",
      "Epoch [1/5], Step [21/3236], Loss: 4.6085, Perplexity: 100.3339\n",
      "Epoch [1/5], Step [22/3236], Loss: 4.3341, Perplexity: 76.2538\n",
      "Epoch [1/5], Step [23/3236], Loss: 4.3762, Perplexity: 79.5362\n",
      "Epoch [1/5], Step [24/3236], Loss: 4.1998, Perplexity: 66.6760\n",
      "Epoch [1/5], Step [25/3236], Loss: 4.3630, Perplexity: 78.4931\n",
      "Epoch [1/5], Step [26/3236], Loss: 4.1873, Perplexity: 65.8425\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8272baa520e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# print info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f'\n\u001b[0;32m---> 30\u001b[0;31m             %(epoch+1, num_epochs, i_step, total_step, loss.data[0], np.exp(loss.data[0]))) \n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# save the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i_step in range(0, total_step):\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        for batch in data_loader:\n",
    "            images, captions = batch[0], batch[1]\n",
    "            break \n",
    "        \n",
    "        # Convert batch of images and captions to Pytorch Variable.\n",
    "        images = to_var(images, volatile=True)\n",
    "        captions = to_var(captions)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Print training statistics.\n",
    "        print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f'\n",
    "            %(epoch+1, num_epochs, i_step, total_step, loss.data[0], np.exp(loss.data[0]))) \n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' %(epoch+1)))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' %(epoch+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
