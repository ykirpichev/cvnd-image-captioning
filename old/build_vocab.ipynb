{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's build some vocab ([link](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/image_captioning/build_vocab.py#L4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main idea behind `class Vocabulary` (use case)\n",
    "\n",
    "- `self.idx` initialized at 0.\n",
    "- loop over words. call `add_word` for each `word` ...\n",
    "    - assign `self.idx2word[self.idx] = word`\n",
    "    - and viceuhversa for `self.word2idx` \n",
    "    - `self.idx += 1`\n",
    "- oh, then `__call__` is used **LATER** as a lookup table\n",
    "- `__len__` is the obvious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### understanding the code that does the stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.53s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "json = 'annotations/captions_train2014.json'\n",
    "coco = COCO(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`coco.anns` is a dict of dicts.\n",
    "\n",
    "```\n",
    "coco.anns = {..., 829717: {'image_id': 133071, 'id': 829717, 'caption': 'A dinner plate has a lemon wedge garnishment.'}, 829719: {'image_id': 238117, 'id': 829719, 'caption': 'A blue camouflage airplane is on a runway.'}}\n",
    "```\n",
    "\n",
    "thus, \n",
    "\n",
    "```\n",
    "coco.anns.keys() = dict_keys([..., 829717, 829719])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'caption': 'A dinner plate has a lemon wedge garnishment.',\n",
       " 'id': 829717,\n",
       " 'image_id': 133071}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco.anns[829717]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A dinner plate has a lemon wedge garnishment.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco.anns[829717]['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'dinner', 'plate', 'has', 'a', 'lemon', 'wedge', 'garnishment', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(coco.anns[829717]['caption'].lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE TO SELF: the period ... do we want it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 2, 'plate': 1, '.': 1, 'wedge': 1, 'lemon': 1, 'garnishment': 1, 'dinner': 1, 'has': 1})\n"
     ]
    }
   ],
   "source": [
    "counter_test = Counter()\n",
    "counter_test.update(tokens)\n",
    "print(counter_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ok we run now ...\n",
    "\n",
    "this code populates a `Counter` object where \n",
    "- `Counter[word]` returns the number of times that `word` appeared in the set of captions\n",
    "- `counter.items()` converts to a list of `(word, count)` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/414113] Tokenized the captions.\n",
      "[100000/414113] Tokenized the captions.\n",
      "[200000/414113] Tokenized the captions.\n",
      "[300000/414113] Tokenized the captions.\n",
      "[400000/414113] Tokenized the captions.\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "ids = coco.anns.keys()\n",
    "for i, id in enumerate(ids):\n",
    "    caption = str(coco.anns[id]['caption'])\n",
    "    tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "    counter.update(tokens)\n",
    "\n",
    "    if i % 100000 == 0:\n",
    "        print(\"[%d/%d] Tokenized the captions.\" %(i, len(ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE TO SELF: _interesting_ that not every caption ends with a period ._."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310919\n",
      "414113\n"
     ]
    }
   ],
   "source": [
    "print(counter['.'])\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 684576), ('.', 310919), ('on', 150675), ('of', 142759), ('the', 137981), ('in', 128909), ('with', 107703), ('and', 98754), ('is', 68686), ('man', 51530)]\n"
     ]
    }
   ],
   "source": [
    "# you know what they say about curiosity\n",
    "print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`words` contains the list of \"words\" that we'll add to the vocab (if `threshold` > 1), then this is less than the total number of words in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum word count threshold\n",
    "threshold = 4\n",
    "# If the word frequency is less than 'threshold', then the word is discarded.\n",
    "words = [word for word, cnt in counter.items() if cnt >= threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get started with making that vocabulary ... by putting the special stuff first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: '<start>', 2: '<end>', 3: '<unk>'}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.add_word('<pad>')\n",
    "vocab.add_word('<start>')\n",
    "vocab.add_word('<end>')\n",
    "vocab.add_word('<unk>')\n",
    "print(vocab.idx2word)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the words to the vocabulary.\n",
    "for i, word in enumerate(words):\n",
    "    vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9952\n",
      "9956\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nice, behaves as expected ... now to define place to save ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = 'data/vocab.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 9956\n",
      "Saved the vocabulary wrapper to 'data/vocab.pkl'\n"
     ]
    }
   ],
   "source": [
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Total vocabulary size: %d\" %len(vocab))\n",
    "print(\"Saved the vocabulary wrapper to '%s'\" %vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sauce, and we're done here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
