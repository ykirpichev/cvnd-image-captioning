{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### Step 1: Get Data Loader for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "# TODO: Define a transform to pre-process the testing images.\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Create the data loader.\n",
    "data_loader = get_loader(transform=transform_test,    \n",
    "                         mode='test')\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# TODO: Specify the saved models to load.\n",
    "encoder_file = 'encoder-5.pkl' \n",
    "decoder_file = 'decoder-5.pkl' \n",
    "\n",
    "# TODO: Select appropriate values for the Python variables below.\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "\n",
    "encoder = EncoderCNN(embed_size)\n",
    "encoder.eval()\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "decoder.eval()\n",
    "\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "ann_id = random.sample(list(coco.anns.keys()), 1)[0]\n",
    "img_id = coco.anns[ann_id]['image_id']\n",
    "image_path = coco.loadImgs(img_id)[0]['file_name']\n",
    "caption = coco.anns[ann_id]['caption']\n",
    "\n",
    "image = Image.open(os.path.join(val_dir, image_path)).convert('RGB')\n",
    "# display image\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "print('Original caption:', caption)\n",
    "\n",
    "image_tensor = transform_val(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "image_var = to_var(image_tensor, volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = encoder(image_var)\n",
    "output = decoder.sample(features)\n",
    "output = output.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_ids = []\n",
    "inputs = features.unsqueeze(1) # 1 x 1 x 256\n",
    "states = None\n",
    "\n",
    "for i in range(20):\n",
    "    hiddens, states = decoder.lstm(inputs, states)\n",
    "    outputs = decoder.linear(hiddens.squeeze(1))\n",
    "    predicted = outputs.max(1)[1]\n",
    "    sampled_ids.append(predicted.data[0])\n",
    "    inputs = decoder.embed(predicted)\n",
    "    inputs = inputs.unsqueeze(1)\n",
    "#sampled_ids = torch.Tensor(sampled_ids)\n",
    "\n",
    "print(sampled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([vocab.idx2word[i] for i in sampled_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
