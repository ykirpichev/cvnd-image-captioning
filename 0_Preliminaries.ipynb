{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you ... xyz.\n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Explore the Data Loader\n",
    "- [Step 2](#step2): Tinker with the CNN Encoder\n",
    "- [Step 3](#step3): Tinker with the RNN Decoder\n",
    "- [Step 4](#step4): Calculate the Loss\n",
    "- [Step 5](#step5): Celebrate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### Step 1: Explore the Data Loader\n",
    "\n",
    "- look at sample images and captions.\n",
    "- tell how data was preprocessed & saved in cocoapi (256 on shortest side)\n",
    "- print some statistics (number of images)\n",
    "- note to self: RETRAIN WITHOUT PAD WORD ASSIGNMENT\n",
    "- show how vocab file is generated\n",
    "- guide their play with `vocab` output\n",
    "- tell should only amend the threshold ... vocab does have other args that should stay at default values. mention `vocab_from_file`\n",
    "- used now to prepare for embedding layer\n",
    "- size of vocab also determines dimensionality of output layer of LSTM & will be used to decode predicted captions. more on that soon\n",
    "- uncomment one of the lines below to examine word2idx, idx2word. will use word2idx to convert images to nums, idx2word later to convert predictions to words ...\n",
    "- show how to get a batch of data\n",
    "- describe how sampler works\n",
    "- note allowed to change data loader, but ok if leave as-is. to pass project, just need to make sure that you can defend your data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/414113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:40<00:00, 10225.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_loader import get_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "# TODO: Set the minimum word count threshold.\n",
    "vocab_threshold = 4\n",
    "\n",
    "# TODO: Define a transform to pre-process the training images.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# TODO: Specify the batch size.\n",
    "batch_size = 10\n",
    "\n",
    "# Create the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when train model later, note don't need to separately populate data loader and vocab. vocab built in to data loader ...\n",
    "- check now that data_loader includes dataset, which includes vocab ... can also get word2idx and idx2word as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of words in vocabulary:', len(data_loader.dataset.vocab))\n",
    "#print(data_loader.dataset.vocab.word2idx)\n",
    "#print(data_loader.dataset.vocab.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- describe how get_train_indices works, where we got the procedure for loading batches from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled indices: [82752, 355776, 164075, 407508, 92747, 376489, 368934, 121907, 231696, 244272]\n",
      "images.shape: torch.Size([10, 3, 224, 224])\n",
      "captions.shape: torch.Size([10, 12])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Randomly sample a caption length, and sample indices with that length.\n",
    "indices = data_loader.dataset.get_train_indices()\n",
    "print('sampled indices:', indices)\n",
    "# Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "data_loader.batch_sampler.sampler = new_sampler\n",
    "    \n",
    "# Obtain the batch.\n",
    "for batch in data_loader:\n",
    "    images, captions = batch[0], batch[1]\n",
    "    break\n",
    "    \n",
    "print('images.shape:',images.shape)\n",
    "print('captions.shape:', captions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "### Step 2: Tinker with the CNN Encoder\n",
    "\n",
    "- make note of module autoreload capability\n",
    "- need to explain to them what `volatile` is for, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "% autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    \"\"\" converts a Pytorch Tensor to a variable and moves to GPU if CUDA is available \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- provide them a CNN encoder.\n",
    "- let them decide if they want to swap out the pre-trained architecture for another (make clear this is allowed). show how to do for vgg-16\n",
    "- consider adding batch normalization if you like ...\n",
    "- make any modifications to the EncoderCNN class you like. use this as a playground to make sure that your model is working well.\n",
    "- Your output should be a Torch Variable with shape `[batch_size, embed_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(features): <class 'torch.autograd.variable.Variable'>\n",
      "features.shape: torch.Size([10, 256])\n",
      "\n",
      "you may proceed\n"
     ]
    }
   ],
   "source": [
    "from model import EncoderCNN\n",
    "\n",
    "# TODO: Specify the dimensionality of the image embedding.\n",
    "embed_size = 256\n",
    "\n",
    "# Initialize the encoder.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "# Move the encoder to GPU if CUDA is available.\n",
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()\n",
    "    \n",
    "# Convert last batch of images (from Step 1) to PyTorch Variable.   \n",
    "images_var = to_var(images, volatile=True)\n",
    "# Pass the images through the encoder.\n",
    "features = encoder(images_var)\n",
    "\n",
    "print('type(features):', type(features))\n",
    "print('features.shape:', features.shape)\n",
    "\n",
    "# Check that your encoder satisfies the requirements of the project.\n",
    "if (type(features)==torch.autograd.variable.Variable) & (features.shape[0]==batch_size) & (features.shape[1]==embed_size):\n",
    "    print('\\nyou may proceed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "### Step 3: Tinker with the RNN Decoder\n",
    "\n",
    "- they have to write the decoder here and I plan to provide a unit test where they can check if implementation is correct\n",
    "- they have to decide hidden size. note embed_size must be same as with encoder, and output must be same as vocab size.\n",
    "- if leave notebook and come back, re-run every cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(outputs): <class 'torch.autograd.variable.Variable'>\n",
      "outputs.shape: torch.Size([10, 12, 9955])\n",
      "\n",
      "you may proceed\n"
     ]
    }
   ],
   "source": [
    "from model import DecoderRNN\n",
    "\n",
    "# TODO: Specify the number of features in the hidden state of the RNN decoder.\n",
    "hidden_size = 512\n",
    "\n",
    "# Store the size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the decoder.\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "# Move the decoder to GPU if CUDA is available.\n",
    "if torch.cuda.is_available():\n",
    "    decoder = decoder.cuda()\n",
    "    \n",
    "# Convert last batch of captions (from Step 1) to a PyTorch Variable. \n",
    "captions_var = to_var(captions)\n",
    "# Pass the encoder output and captions through the decoder.\n",
    "outputs = decoder(features, captions_var)\n",
    "\n",
    "print('type(outputs):', type(outputs))\n",
    "print('outputs.shape:', outputs.shape)\n",
    "\n",
    "# Check that your decoder satisfies the requirements of the project.\n",
    "if (type(outputs)==torch.autograd.variable.Variable) & (outputs.shape[0]==batch_size) & (outputs.shape[1]==captions.shape[1]) & (outputs.shape[2]==vocab_size):\n",
    "    print('\\nyou may proceed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "### Step 4: Calculate the Loss\n",
    "\n",
    "- i give the criterion. students have to calculate the loss\n",
    "- still trying to figure out how to best assess this. maybe put in py file that's not hidden? tell students not to look unless they really need to? will appear in the next notebook anyway. may as well put it in a file that's not hidden ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: Calculate the batch loss.\n",
    "loss = criterion(outputs.view(-1, vocab_size), captions_var.view(-1))\n",
    "\n",
    "print('loss:', loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "### Step 5: Celebrate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have completed all of the steps of this notebook, you're ready to move on to the next notebook in the project sequence.\n",
    "\n",
    "When you are done with this notebook, please navigate to **1_Train.ipynb**, where you will train your own CNN-RNN model to generate image captions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
