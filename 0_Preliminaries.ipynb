{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "In this notebook, some template code has already been provided for you ...\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will xyz.\n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "\n",
    "(insert table of contents here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Explore the Dataset\n",
    "\n",
    "- look at sample images and captions.\n",
    "- tell how data was preprocessed & saved in cocoapi (256 on shortest side)\n",
    "- print some statistics (number of images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Examine the Data Loader\n",
    "\n",
    "- note to self: RETRAIN WITHOUT PAD WORD ASSIGNMENT\n",
    "- show how vocab file is generated\n",
    "- guide their play with `vocab` output\n",
    "- tell should only amend the threshold ... vocab does have other args that should stay at default values. mention `vocab_from_file`\n",
    "- used now to prepare for embedding layer\n",
    "- size of vocab also determines dimensionality of output layer of LSTM & will be used to decode predicted captions. more on that soon\n",
    "- uncomment one of the lines below to examine word2idx, idx2word. will use word2idx to convert images to nums, idx2word later to convert predictions to words ...\n",
    "- show how to get a batch of data\n",
    "- describe how sampler works\n",
    "- note allowed to change data loader, but ok if leave as-is. to pass project, just need to make sure that you can defend your data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n"
     ]
    }
   ],
   "source": [
    "from vocabulary import Vocabulary\n",
    "vocab = Vocabulary(4, vocab_from_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.50s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.47s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1015/414113 [00:00<00:40, 10140.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:41<00:00, 10030.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_loader import get_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "# specify vocab threshold\n",
    "vocab_threshold = 4\n",
    "\n",
    "# image preprocessing\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# specify batch size\n",
    "batch_size = 10\n",
    "\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when train model later, note don't need to separately populate data loader and vocab. vocab built in to data loader ...\n",
    "- check now that data_loader includes dataset, which includes vocab ... can also get word2idx and idx2word as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 9955\n"
     ]
    }
   ],
   "source": [
    "print('Number of words in vocabulary:', len(data_loader.dataset.vocab))\n",
    "#print(data_loader.dataset.vocab.word2idx)\n",
    "#print(data_loader.dataset.vocab.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- describe how get_train_indices works, where we got the procedure for loading batches from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "indices = data_loader.dataset.get_train_indices()\n",
    "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "for batch in data_loader:\n",
    "    images, captions = batch[0], batch[1]\n",
    "    break\n",
    "    \n",
    "print('sampled indices:', data_loader.batch_sampler.sampler.indices)\n",
    "print('images.shape:',images.shape)\n",
    "print('captions.shape:', captions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Tinker with the CNN Encoder\n",
    "\n",
    "- provide them a CNN encoder.\n",
    "- let them decide if they want to swap out the pre-trained architecture for another (make clear this is allowed). show how to do for vgg-16\n",
    "- make note of module autoreload capability\n",
    "- encourage them to stop here and think about dimensionality of output\n",
    "- need to explain to them what `volatile` is for, etc\n",
    "- consider adding batch normalization if you like ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from model import EncoderCNN\n",
    "\n",
    "# amend this if you like\n",
    "embed_size = 256\n",
    "\n",
    "encoder = EncoderCNN(embed_size)\n",
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x, volatile=False):\n",
    "    \"\"\" converts a Pytorch Tensor to a variable and moves to GPU if CUDA is available \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "images_var = to_var(images, volatile=True)\n",
    "features = encoder(images_var)\n",
    "\n",
    "print('features.shape:', features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Tinker with RNN Decoder\n",
    "\n",
    "- they have to write the decoder here and I plan to provide a unit test where they can check if implementation is correct\n",
    "- they have to decide hidden size, embed size - won't provide default values but will provide papers they can read\n",
    "- may need to use vocabulary.  note `len(vocab)` returns the total number of tokens\n",
    "- if leave notebook and come back, re-run every cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DecoderRNN\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "if torch.cuda.is_available():\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_var = to_var(captions)\n",
    "outputs = decoder(features, captions_var)\n",
    "\n",
    "print('outputs.shape:', outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Calculate the Loss\n",
    "\n",
    "- i give the criterion. students have to calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: \n",
    "loss = criterion(outputs.view(-1, vocab_size), captions_var.view(-1))\n",
    "\n",
    "print('loss:', loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Congratulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have completed all of the steps of this notebook, you're ready to move on to the next notebook in the project sequence.\n",
    "\n",
    "When you are done with this notebook, please navigate to **1_Train.ipynb**, where you will train your own CNN-RNN model to generate image captions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
