{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "In this notebook, some template code has already been provided for you ...\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will xyz.\n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "\n",
    "(insert table of contents here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Explore the Dataset\n",
    "\n",
    "- look at sample images and captions.\n",
    "- tell how data was preprocessed & saved in cocoapi (256 on shortest side)\n",
    "- print some statistics (number of images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the Vocabulary\n",
    "\n",
    "- note to self: RETRAIN WITHOUT PAD WORD ASSIGNMENT\n",
    "- show how vocab file is generated\n",
    "- guide their play with `vocab` output\n",
    "- tell them what they can amend, and what they cannot amend // really they should only be able to amend the threshold ...\n",
    "- how to delete / override existing file\n",
    "- used now to prepare for embedding layer\n",
    "- size of vocab also determines dimensionality of output layer of LSTM & will be used to decode predicted captions. more on that soon\n",
    "- provide option to override existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocabulary import Vocabulary\n",
    "\n",
    "vocab_file = './vocab.pkl'\n",
    "pad_word = \"<pad>\"\n",
    "start_word = \"<start>\"\n",
    "end_word = \"<end>\"\n",
    "unk_word = \"<unk>\",\n",
    "vocab_threshold = 4\n",
    "captions_file = '../cocoapi/annotations/captions_train2014.json'\n",
    "\n",
    "vocab = Vocabulary(vocab_file, pad_word, start_word, end_word, unk_word, vocab_threshold, captions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare the Data Loader\n",
    "\n",
    "- note to self: may need to RE-ORG these files ... especially to separate data loader from caption lengths\n",
    "- show how to get a batch of data\n",
    "- describe how sampler works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.60s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 861/414113 [00:00<00:48, 8606.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:41<00:00, 10047.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_loader import get_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "# image preprocessing\n",
    "transform = transforms.Compose([ \n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "data_loader, caption_lengths = get_loader(transform=transform,\n",
    "                                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected length: 10\n",
      "sampled indices: [350874, 146864, 48975, 255536, 114865, 222642, 111681, 181747, 52278, 289845]\n",
      "images.shape: torch.Size([10, 3, 224, 224])\n",
      "captions.shape: torch.Size([10, 12])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "sel_length = np.random.choice(caption_lengths)\n",
    "all_indices = np.where([caption_lengths[i] == sel_length for i in np.arange(len(caption_lengths))])[0]\n",
    "indices = list(np.random.choice(all_indices, size=batch_size))\n",
    "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "for batch in data_loader:\n",
    "    images, captions = batch[0], batch[1]\n",
    "    break\n",
    "    \n",
    "print('selected length:', sel_length)\n",
    "print('sampled indices:', data_loader.batch_sampler.sampler.indices)\n",
    "print('images.shape:',images.shape)\n",
    "print('captions.shape:', captions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Tinker with the CNN Encoder\n",
    "\n",
    "- provide them a CNN encoder.\n",
    "- let them decide if they want to swap out the pre-trained architecture for another (make clear this is allowed). show how to do for vgg-16\n",
    "- make note of module autoreload capability\n",
    "- encourage them to stop here and think about dimensionality of output\n",
    "- need to explain to them what `volatile` is for, etc\n",
    "- consider adding batch normalization if you like ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from model import EncoderCNN\n",
    "\n",
    "# amend this if you like\n",
    "embed_size = 100\n",
    "\n",
    "encoder = EncoderCNN(embed_size)\n",
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.shape: torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "images_var = to_var(images, volatile=True)\n",
    "features = encoder(images_var)\n",
    "\n",
    "print('features.shape:', features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Tinker with RNN Decoder\n",
    "\n",
    "- they have to write the decoder here and I plan to provide a unit test where they can check if implementation is correct\n",
    "- they have to decide hidden size, embed size - won't provide default values but will provide papers they can read\n",
    "- may need to use vocabulary.  note `len(vocab)` returns the total number of tokens\n",
    "- if leave notebook and come back, re-run every cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DecoderRNN\n",
    "\n",
    "embed_size = 100\n",
    "hidden_size = 30\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "if torch.cuda.is_available():\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape: torch.Size([10, 12, 9956])\n"
     ]
    }
   ],
   "source": [
    "captions_var = to_var(captions)\n",
    "outputs = decoder(features, captions_var)\n",
    "\n",
    "print('outputs.shape:', outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Calculate the Loss\n",
    "\n",
    "- i give the criterion. students have to calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.227032661437988\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(outputs.view(-1, output_size), captions_var.view(-1))\n",
    "\n",
    "print('loss:', loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Congratulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have completed all of the steps of this notebook, you're ready to move on to the next notebook in the project sequence.\n",
    "\n",
    "When you are done with this notebook, please navigate to **1_Train.ipynb**, where you will train your own CNN-RNN model to generate image captions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
